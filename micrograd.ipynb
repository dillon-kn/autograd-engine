{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = np.array(data)\n",
    "        self.grad = np.zeros_like(self.data) # gradient\n",
    "        self._backward = lambda: None # function for backward pass of gradient\n",
    "        self._prev = set(_children) # set of parent nodes\n",
    "        self._op = _op # operation used to calculate\n",
    "        self.label = label # label for the node\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\" # used to print object info\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        # Allow for adding of non-Value objects to Value objects\n",
    "        other = other if isinstance(other, Value) else Value(other) # if other not Value, make it a Value\n",
    "        out = Value(self.data + other.data, (self, other), '+') # create new Value object\n",
    "\n",
    "        def _backward():\n",
    "            # += because the nodes may be attached to multiple others\n",
    "            self.grad += out.grad # just take on the gradient of the output, since it's addition\n",
    "            other.grad += out.grad # same\n",
    "        out._backward = _backward # assign backward function to other node's _backward attribute\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1 # return the additive inverse for subtraction purposes\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other) # add the opposite\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        # Allow for multiplication of non-Value objects to Value objects\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            # += because the nodes may be attached to multiple others\n",
    "            self.grad += other.data * out.grad # multiply gradient by the other data, which we treat as constant\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        # For when the left-side object is not a Value objectk\n",
    "        return self * other\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * other ** -1\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"Currently only supporting int or float\" # make sure data type is correct for power\n",
    "        out = Value(self.data ** other, (self,), f'**{other}') # create output Value object\n",
    "        def _backward():\n",
    "            self.grad += other * self.data ** (other - 1) * out.grad # power rule + chain rule\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def exp(self): # This is e^x, not the exponential function\n",
    "        x = self.data \n",
    "        out = Value(np.exp(x), (self,), 'exp') # Create new object for the output\n",
    "        def _backward():\n",
    "            self.grad += out.grad * out.data # e^x is in out, then apply chain rule\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        t = np.tanh(self.data)\n",
    "        out = Value(t, (self, ), 'tanh')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def sigmoid(self):\n",
    "        return 1 / (1 + (-self).exp())\n",
    "\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = [] # init list to contain all nodes\n",
    "        visited = set()\n",
    "        # recursively build tree by adding node children to queue, adding current node to topo\n",
    "        # then running build topo on each child in queue\n",
    "        # note: it's a dfs construction\n",
    "        def build_topo(v): \n",
    "            if v not in visited: \n",
    "                visited.add(v)\n",
    "                for child in v._prev: \n",
    "                    build_topo(child) # run build topo on children\n",
    "                topo.append(v) # add current node to topo\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        # starting from the very first nodes, run backward function on each\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "    # build set of all nodes and edges in graph\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "    \n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n)) # get mem address of node\n",
    "        # for any value in graph, create rectangular record node for it\n",
    "        dot.node(name=uid, label=\"{%s | data %.4f | grad %.4f}\" % (n.label, n.data, n.grad, ), shape='record')\n",
    "        if n._op:\n",
    "            # if value is result of operation, create op node for it\n",
    "            dot.node(name=uid+n._op, label=n._op)\n",
    "            # connect node to value\n",
    "            dot.edge(uid+n._op, uid)\n",
    "    for n1, n2 in edges:\n",
    "        # connect n1 to op node of n2\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "        \n",
    "    return dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071066904050358\n",
      "---\n",
      "x2 0.5000001283844369\n",
      "w2 0.0\n",
      "x1 -1.5000003851533106\n",
      "w1 1.0000002567688737\n"
     ]
    }
   ],
   "source": [
    "# doing the same exact thing but in pytorch\n",
    "import torch\n",
    "\n",
    "x1 = torch.Tensor([2.0]).double()                ; x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double()                ; x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double()               ; w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double()                ; w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double()  ; b.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print('---')\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(np.random.uniform(-1, 1)) for _ in range(nin)] # weights\n",
    "        self.b = Value(np.random.uniform(-1, 1)) # bias\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # dot product of weights and inputs + bias\n",
    "        return sum((wi*xi for wi, xi in zip(self.w, x)), self.b).tanh()\n",
    "\n",
    "class Layer:\n",
    "\n",
    "    def __init__(self, nin, nout): # nin = # of inputs, # nout = # of neurons in layer\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)] # create nout neurons, each with nin inputs\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outputs = [n(x) for n in self.neurons] # gets the activation of each neuron in the layer\n",
    "        return outputs[0] if len(outputs) == 1 else outputs # if list has only one element, just return the element\n",
    "\n",
    "class MLP: # multi-layer perceptron\n",
    "\n",
    "    def __init__(self, nin, nouts): # nin = # of inputs, nouts = list of # of neurons in each layer\n",
    "        size = [nin] + nouts # array of # of neurons in each layer\n",
    "        self.layers = [Layer(size[i], size[i+1]) for i in range(len(nouts))] # create layers pairwise\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers: # feed forward through each layer\n",
    "            x = layer(x) # get activations of each neuron in the layer\n",
    "        return x if isinstance(x, Value) else Value(x)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for neuron in layer.neurons for p in neuron.w + [neuron.b]]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "\n",
    "# x=[2.0, 3.0, -1.0]\n",
    "# n = MLP(3, [4, 4, 1])\n",
    "# print(n(x))\n",
    "# draw_dot(n(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# To validate amino acid sequencing data\n",
    "def is_valid_sequence(seq):\n",
    "    valid_chars = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "    return all(char in valid_chars for char in seq.upper())\n",
    "\n",
    "# Load, parse, and preprocess data\n",
    "data = pd.read_excel(\"general_amps.xlsx\")\n",
    "\n",
    "# Filter out non-string sequences and invalid sequences\n",
    "valid_data = [(seq.upper(), label) for seq, label in zip(data['Sequence'], data['Activity']) \n",
    "              if isinstance(seq, str) and is_valid_sequence(seq)]\n",
    "\n",
    "# Split into sequences and labels\n",
    "sequences, labels = zip(*valid_data)\n",
    "sequences = [list(seq) for seq in sequences]\n",
    "labels = [1 if isinstance(label, str) and 'antimicrobial' in label.lower() else 0 for label in labels]\n",
    "\n",
    "# Find the maximum sequence length\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "# Identify all unique characters in the sequences\n",
    "unique_chars = sorted(set(\"ACDEFGHIKLMNPQRSTVWY\"))\n",
    "unique_chars.append('')  # Add empty string for padding\n",
    "\n",
    "# Pad sequences to the same length for fixed input size to nn\n",
    "padded_sequences = [seq + [''] * (max_length - len(seq)) for seq in sequences]\n",
    "\n",
    "# Encode sequences using one-hot encodings\n",
    "encoder = OneHotEncoder(sparse_output=False, categories=[unique_chars]*max_length)\n",
    "encoded_sequences = encoder.fit_transform(padded_sequences)\n",
    "\n",
    "# Split data into 70% train, 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_sequences, labels, test_size=0.3, random_state=RANDOM_STATE)\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "# Split testing data into half validation half test, so 15% validation and 15% testing data\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model size\n",
    "input_size = encoded_sequences.shape[1]\n",
    "hidden_size = 64  # You can adjust this\n",
    "output_size = 1  # anti-microbial or not\n",
    "model = MLP(input_size, [hidden_size, output_size]) # input_size -> 64 -> 1\n",
    "\n",
    "# Define loss function\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-15 # for safe logarithmic calculations\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon) # keep y_pred between eps and 1-eps\n",
    "    # Binary cross entropy loss function - common practice for binary classification\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Define training loop\n",
    "def train(model, X, y, learning_rate=0.01, epochs=100, batch_size=32):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Create mini-batches\n",
    "        permutation = np.random.permutation(len(X))\n",
    "        X_shuffled = X[permutation]\n",
    "        y_shuffled = y[permutation]\n",
    "\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = np.array([model(x).data for x in X_batch])\n",
    "\n",
    "            # Compute loss\n",
    "            loss = binary_cross_entropy(y_batch, y_pred)\n",
    "\n",
    "            # Backward pass\n",
    "            model.zero_grad()\n",
    "            for j in range(len(X_batch)):\n",
    "                model(X_batch[j]).backward()\n",
    "\n",
    "            # Update weights\n",
    "            for p in model.parameters():\n",
    "                p.data -= learning_rate * p.grad / len(X_batch)\n",
    "\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss/len(X)}\")\n",
    "\n",
    "# Function to evaluate accuracy\n",
    "def evaluate(model, X, y):\n",
    "    correct = 0\n",
    "    total = len(X)\n",
    "    for x, y_true in zip(X, y):\n",
    "        y_pred = model(x)\n",
    "        if (y_pred.data > 0.5) == y_true:\n",
    "            correct += 1\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tanh() takes from 1 to 2 positional arguments but 0 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train(model, X_train, y_train)\n",
      "Cell \u001b[0;32mIn[9], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, X, y, learning_rate, epochs, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_shuffled[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([model(x)\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_batch])\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m binary_cross_entropy(y_batch, y_pred)\n",
      "Cell \u001b[0;32mIn[9], line 30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_shuffled[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([model(x)\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_batch])\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m binary_cross_entropy(y_batch, y_pred)\n",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m, in \u001b[0;36mMLP.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers: \u001b[38;5;66;03m# feed forward through each layer\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x) \u001b[38;5;66;03m# get activations of each neuron in the layer\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 17\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [n(x) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons] \u001b[38;5;66;03m# gets the activation of each neuron in the layer\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m outputs\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 17\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [n(x) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons] \u001b[38;5;66;03m# gets the activation of each neuron in the layer\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m outputs\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mNeuron.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# dot product of weights and inputs + bias\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m((wi\u001b[38;5;241m*\u001b[39mxi \u001b[38;5;28;01mfor\u001b[39;00m wi, xi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw, x)), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb)\u001b[38;5;241m.\u001b[39mtanh()\n",
      "Cell \u001b[0;32mIn[4], line 70\u001b[0m, in \u001b[0;36mValue.tanh\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtanh\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 70\u001b[0m     t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtanh()\n\u001b[1;32m     71\u001b[0m     out \u001b[38;5;241m=\u001b[39m Value(t, (\u001b[38;5;28mself\u001b[39m, ), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_backward\u001b[39m():\n",
      "\u001b[0;31mTypeError\u001b[0m: tanh() takes from 1 to 2 positional arguments but 0 were given"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "train(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "# train_accuracy = evaluate(model, X_train, y_train)\n",
    "# val_accuracy = evaluate(model, X_val, y_val)\n",
    "test_accuracy = evaluate(model, X_test, y_test)\n",
    "\n",
    "# print(f\"Train Accuracy: {train_accuracy}\")\n",
    "# print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
